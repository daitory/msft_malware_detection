import pandas as pd
import numpy  as np 
import io
import multicore
import time

start=time.time()
dataframe=pd.read_csv(filepath_or_buffer="D:\\Documents\\Data Science Certificate\\group_assignment\\microsoft-malware-prediction\\train.csv",sep=",",low_memory=False)
#data_project.info()
end=time.time()
print((end-start)/60)
print("time elapsed :  ",(end-start)/60)
start=time.time()

startingMemoryUsage = dataframe.memory_usage().sum() / 1024**2

print('Dataframe was {:.2f} MB'.format(startingMemoryUsage))

for column in dataframe.columns:
    columnDataType = dataframe[column].dtype

    if columnDataType != object:
        columnMin = dataframe[column].min()
        columnMax = dataframe[column].max()

        if str(columnDataType)[:3] == 'int':
            if columnMin > np.iinfo(np.int8).min and columnMax < np.iinfo(np.int8).max:
                dataframe[column] = dataframe[column].astype(np.int8)
            elif columnMin > np.iinfo(np.int16).min and columnMax < np.iinfo(np.int16).max:
                dataframe[column] = dataframe[column].astype(np.int16)
            elif columnMin > np.iinfo(np.int32).min and columnMax < np.iinfo(np.int32).max:
                dataframe[column] = dataframe[column].astype(np.int32)
            elif columnMin > np.iinfo(np.int64).min and columnMax < np.iinfo(np.int64).max:
                dataframe[column] = dataframe[column].astype(np.int64)  
        else:
            if columnMin > np.finfo(np.float16).min and columnMax < np.finfo(np.float16).max:
                dataframe[column] = dataframe[column].astype(np.float16)
            elif columnMin > np.finfo(np.float32).min and columnMax < np.finfo(np.float32).max:
                dataframe[column] = dataframe[column].astype(np.float32)
            else:
                dataframe[column] = dataframe[column].astype(np.float64)
    else:
        dataframe[column] = dataframe[column].astype('category')

endingingMemoryUsage = dataframe.memory_usage().sum() / 1024**2

print('Dataframe is now: {:.2f} MB'.format(endingingMemoryUsage))

end=time.time()
print((end-start)/60)
print("time elapsed :  ",(end-start)/60)


###############################################################################################################################################################################
#                                                                   DATA CLEANING 
###############################################################################################################################################################################



data_project=dataframe
print("Quantity of computers with a malware infection %7.1f " % data_project['HasDetections'].sum())
print("Quantity  of computers analyzed %7.1f" % data_project['HasDetections'].count())
print("Percentage of problems of the overall machines analyzed %1.3f " % (data_project['HasDetections'].sum()/data_project['HasDetections'].count()))

print(np.sort(data_project.isnull().sum()/data_project.shape[0]))

col_drop=[data_project.columns[i] for i in range(0,data_project.shape[1])  if((data_project.iloc[:,i].isnull().sum()/data_project.shape[0])>=0.3)  ]
print("Columns that are going to be removed",col_drop)

data_project1=data_project.drop(col_drop,axis=1)
data_project1.head(5)


#FILLING VARIABLES  - AS I CORROBORATE WE CAN ERRASE THE RAM PROBLEMS (AS WE COUNT WITH 8.9MM dataset , if we do that we can eliminate the combination of RAM  and processor core count missing)
data_project1[['Census_TotalPhysicalRAM','Census_ProcessorCoreCount','Processor','Census_InternalPrimaryDiagonalDisplaySizeInInches','AVProductsInstalled','Platform']].info()
print(data_project1[['Census_TotalPhysicalRAM','Census_ProcessorCoreCount','Processor','Census_InternalPrimaryDiagonalDisplaySizeInInches','AVProductsInstalled','Platform']].isnull().sum())
print(data_project1[['Census_TotalPhysicalRAM','Census_ProcessorCoreCount','Processor','Census_InternalPrimaryDiagonalDisplaySizeInInches','AVProductsInstalled','Platform']].isnull().sum()/data_project1.shape[0])

# THIS OPTION WAS NOT USED (THE FILLED OF MEMORY)
# THERE IS A CHANCE TO FILL THE AMOUNT OF MEMORY BASE IN : THOSE ARM (ARE TABLETS SO THEY COULD BE BOUND TO THE MORE COMMON TABLET MEMORY, IN THE CASE OF 64 BITS COULD BE ATTACHED TO THE RAM OVER 4 GB INCLUSIVE WITH MORE QUANTITY THE SAME AS X86 (32BITS ) WITH LESS THAN 4 GB - MORE COMMON DATA)

data_project1['Census_TotalPhysicalRAM'][data_project1['Census_TotalPhysicalRAM'].isnull()]
print("almost all the processor core count null are associated with total phisical RAM null: %i " % (data_project1['Census_ProcessorCoreCount'][data_project1['Census_TotalPhysicalRAM'].isnull() ].isnull().sum()))
#this is going to erase the memory missings (there are so a lot of filled with memory so we are just losing 1% of the data with this cleaning)
data_project2=data_project1[data_project1['Census_TotalPhysicalRAM'].isnull()==False] #NAN FILTER FOR MEMORY
print(data_project2[['Census_TotalPhysicalRAM','Census_ProcessorCoreCount','Processor','Census_InternalPrimaryDiagonalDisplaySizeInInches','AVProductsInstalled','Platform']].isnull().sum())
print(data_project2[['Census_TotalPhysicalRAM','Census_ProcessorCoreCount','Processor','Census_InternalPrimaryDiagonalDisplaySizeInInches','AVProductsInstalled','Platform']].isnull().sum()/data_project2.shape[0])
data_project2['Processor'][data_project2['Census_ProcessorCoreCount'].isnull()].unique()


start=time.time()
value_64=np.array((data_project2['Census_ProcessorCoreCount'][(data_project2['Census_ProcessorCoreCount']>4)  * (data_project2['Processor']=='x64') ])).mean()
value_32=np.array((data_project2['Census_ProcessorCoreCount'][(data_project2['Census_ProcessorCoreCount']<=4)  * (data_project2['Processor']=='x86') ])).mean()
value_arm64=np.array((data_project2['Census_ProcessorCoreCount'][ (data_project2['Processor']=='arm64') *  (data_project2['Census_ProcessorCoreCount'].isnull()==False) ])).mean() 

data_project2['ProcessorCoreCount_4missing']=data_project2[['Processor','Census_ProcessorCoreCount']].apply(lambda x: value_64 if  (x.loc['Processor']=='x64') else ( value_32  if ((x.loc['Processor']=='x86') ) else (value_arm64 ) ),axis=1)
end=time.time()

print("elapsed time",(end-start)/60)
#I have to check the results that comes out from the preceding lambda 


data_project2['filled_ProcessorCoreCount']=data_project2[['ProcessorCoreCount_4missing','Census_ProcessorCoreCount']].apply(lambda x: x.loc['ProcessorCoreCount_4missing'] if  (np.isnan(x.loc['Census_ProcessorCoreCount'])) else x.loc['Census_ProcessorCoreCount'],axis=1)

data_project2['filled_ProcessorCoreCount'].isnull().sum()

#THERE IS NO NAN's in this variable
#####################################################################################################################################
# PRIMARY DISK TYPE NAME
#THE PRIMARY DISK WOULD BE IMPORTANT ALSO
data_project2['Census_PrimaryDiskTypeName'] = data_project2['Census_PrimaryDiskTypeName'].replace('Unspecified', 'UNKNOWN')
data_project2['Census_PrimaryDiskTypeName'].fillna('UNKNOWN',inplace=True)
data_project2['Census_PrimaryDiskTypeName'].unique()


data_project2['Census_InternalPrimaryDiagonalDisplaySizeInInches'].unique()

data_project2['Census_ProcessorModelIdentifier']

###################################################################################################################################################################################
#THIS PART IS GOING TO BE USED FOR FILL THE MISSINGS IN THE REST OF THE DATA
data_project2['IsProtected'].unique()
data_project2['GeoNameIdentifier'].unique()
data_project2['AVProductStatesIdentifier'].describe()
data_project2['Census_ProcessorModelIdentifier'].describe()
data_project2['IeVerIdentifier'].describe()

data_project2['CityIdentifier'].fillna(0.0, inplace = True) 
data_project2['OrganizationIdentifier'].fillna(0.0, inplace = True) 
data_project2['GeoNameIdentifier'].fillna(0.0, inplace = True) # the 0.0 means that the data was unknown 
data_project2['IsProtected'].fillna(-1.0, inplace = True) # the -1 means that the data was unknown 
data_project2['IeVerIdentifier'].fillna(0.0, inplace = True) # the 00 means that the data was unknown
data_project2['AVProductStatesIdentifier'].fillna(0.0, inplace = True) # the 00 means that the data was unknown
# data_project2['Census_ProcessorCoreCount'].fillna(0.0, inplace = True)
data_project2['Census_OEMNameIdentifier'].fillna(0.0, inplace = True) 
data_project2['Census_OEMModelIdentifier'].fillna(0.0, inplace = True)
data_project2['Census_ProcessorModelIdentifier'].fillna(0.0, inplace = True)# the 0.0 means that the data was unknown
# data_project2['Census_PrimaryDiskTotalCapacity'].fillna(0.0, inplace = True)
# data_project2['Census_SystemVolumeTotalCapacity'].fillna(0.0, inplace = True)
# data_project2['Census_TotalPhysicalRAM'].fillna(0.0, inplace = True)
data_project2['Census_ChassisTypeName'].fillna("0", inplace = True)
data_project2['Census_InternalPrimaryDiagonalDisplaySizeInInches'].fillna(0.0, inplace = True)
data_project2['Census_InternalPrimaryDisplayResolutionHorizontal'].fillna(0.0, inplace = True)
data_project2['Census_InternalPrimaryDisplayResolutionVertical'].fillna(0.0, inplace = True)
data_project2['Census_OSInstallLanguageIdentifier'].fillna(0.0, inplace = True)
data_project2['Census_FirmwareManufacturerIdentifier'].fillna(0.0, inplace = True)
data_project2['Census_FirmwareVersionIdentifier'].fillna(0.0, inplace = True)
data_project2['Wdft_RegionIdentifier'].fillna(0.0, inplace = True)


############################################################################################################
# CHASIS TYPE NAMES 

data_project2['Census_ChassisTypeName'] = data_project2['Census_ChassisTypeName'].replace(['0', '30', '35', '31', '88', '32', '127', '25', '44', '36', '81', '28', '112', '39', '45', '49', '82', '76'],['UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN'])


############################################################################################################
# Census_PowerPlatformRoleName - Replace null values with "UNKNOWN"

data_project2['Census_PowerPlatformRoleName'].fillna("UNKNOWN", inplace = True) 
data_project2['Census_PowerPlatformRoleName'] = data_project2['Census_PowerPlatformRoleName'].replace("Unspecified", "UNKNOWN")

##############################################################################################################


data_project2['AVProductsInstalled'].fillna(data_project2['AVProductsInstalled'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['AVProductsEnabled'].fillna(data_project2['AVProductsEnabled'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['OsBuildLab'].fillna(data_project2['OsBuildLab'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['SMode'].fillna(data_project2['SMode'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['Firewall'].fillna(data_project2['Firewall'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['Census_ProcessorManufacturerIdentifier'].fillna(data_project2['Census_ProcessorManufacturerIdentifier'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
#data_project2['Census_PrimaryDiskTypeName'].fillna(data_project2['Census_PrimaryDiskTypeName'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['Census_PrimaryDiskTotalCapacity'].fillna(data_project2['Census_PrimaryDiskTotalCapacity'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['Census_SystemVolumeTotalCapacity'].fillna(data_project2['Census_SystemVolumeTotalCapacity'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['Census_HasOpticalDiskDrive'].fillna(data_project2['Census_HasOpticalDiskDrive'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['Census_IsFlightsDisabled'].fillna(data_project2['Census_IsFlightsDisabled'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['Census_IsAlwaysOnAlwaysConnectedCapable'].fillna(data_project2['Census_IsAlwaysOnAlwaysConnectedCapable'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['Census_IsVirtualDevice'].fillna(data_project2['Census_IsVirtualDevice'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['Wdft_IsGamer'].fillna(data_project2['Wdft_IsGamer'].value_counts().sort_values(ascending=False).index[0], inplace = True) 
data_project2['Census_InternalBatteryNumberOfCharges'].fillna(data_project2['Census_InternalBatteryNumberOfCharges'].value_counts().sort_values(ascending=False).index[0], inplace = True) 

#################################################################################################################

data_project2['RtpStateBitfield'].fillna(1.0, inplace = True) 
data_project2['RtpStateBitfield'] = data_project2['RtpStateBitfield'].replace([3.0, 5.0, 7.0, 8.0, 35.0], [1.0, 1.0, 1.0, 1.0, 1.0])


###################################################################################################################
# UacLuaenable - Replace null and non-zero values with 1

data_project2['UacLuaenable'].fillna(1.0, inplace = True) 
data_project2['UacLuaenable'] = data_project2['UacLuaenable'].replace([48.0, 2.0, 49.0, 6357062.0, 3.0, 5.0, 16777216.0, 7798884.0, 255.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])

#######################################################################################################################
#                                                    NEW VARIABLES 
###############################################################################################################################



# ALSO WERE SO IMPORTANT THE GAMING CAPABILITIES AND THE PRIMARY DISK CAPACITY

###############################################################################################################################################
# NEW VARIABLES 

data_project2['DANGEROUS_RAM_INSTALLED']=data_project2[['Census_TotalPhysicalRAM']].apply(lambda x: 1 if (x['Census_TotalPhysicalRAM']<=(1024*16) and (1024*4)<x['Census_TotalPhysicalRAM'])  else 0,axis=1)

data_project2['DANGEROUS_PROCESSOR_INSTALLED']=data_project2[['filled_ProcessorCoreCount']].apply(lambda x: 1 if (x['filled_ProcessorCoreCount']<=32 and 3<x['filled_ProcessorCoreCount']) else 0,axis=1)
    
data_project2['DANGEROUS_PROCESSOR_TYPE_INSTALLED']=data_project2[['Processor']].apply(lambda x: 1 if (x['Processor']=='x64') else 0,axis=1)

data_project2['DANGEROUS_DISPLAY_SIZE']=data_project2[['Census_InternalPrimaryDiagonalDisplaySizeInInches']].apply(lambda x: 1 if (x['Census_InternalPrimaryDiagonalDisplaySizeInInches']<=60 and x['Census_InternalPrimaryDiagonalDisplaySizeInInches']>=17 ) else 0,axis=1)

data_project2['HIGH_END_COMPUTER']=data_project2[['DANGEROUS_RAM_INSTALLED','DANGEROUS_PROCESSOR_INSTALLED','DANGEROUS_DISPLAY_SIZE']].apply(lambda x: 1 if (x['DANGEROUS_RAM_INSTALLED']==1 and  x['DANGEROUS_PROCESSOR_INSTALLED']==1) else 0,axis=1)


###################################################################################################################
# SECURITY RELATED

data_project2['HAS_ONLY_ONE_AV']=data_project2[['AVProductsInstalled']].apply(lambda x: 1 if (x['AVProductsInstalled']==1 ) else 0,axis=1)


data_project2['HAS_AN_OS_WSUPPORT']=data_project2[['Platform']].apply(lambda x: 1 if (x['Platform']=='windows10' or x['Platform']=='windows8' ) else 0,axis=1)

##########################################################################################################################################################################################

#REVISION

print(data_project2.isnull().sum()/data_project2.shape[0])

print("max number of null categories caught",max(data_project2.isnull().sum()))

def print_full(df):
    pd.set_option('display.max_rows', len(df))
    print(df)
    pd.reset_option('display.max_rows')



data_project2['Census_ProcessorModelIdentifier']
data_project2['Census_PrimaryDiskTotalCapacity'] # It could be filled with the mean or the most common capacity 
data_project2['Census_SystemVolumeTotalCapacity']# This could be at much the number declare in Census_PrimaryDiskTotalCapacity. (it depends on disk total capacity and usage)
#there should be note that the amount of missings of both variables is almost the same. So the way that they are going to be filled must be similar. (they could be removed also but we removed 1% of the data)
data_project2['Census_PrimaryDiskTotalCapacity'].isnull().sum()
data_project2['Census_SystemVolumeTotalCapacity'].isnull().sum()

print(data_project2.columns[data_project2.isnull().sum()>0])

print(data_project2.columns[data_project2.isnull().sum()>0])
data_project3=data_project2.drop('Census_ProcessorCoreCount',axis=1)

print(data_project3.columns[data_project3.isnull().sum()>0])


########################################################################################################################################################################
#                                              UNIVARIATE ANALISYS                                   
########################################################################################################################################################################

# RELEASE MEMORY 
import gc
gc.collect()

# #######################################################
# # GINI AND LOGIT_TRANSFORMATION (CALLED LN)

data_project3_1=data_project3.drop('ProcessorCoreCount_4missing',axis=1)
data_project3_1.to_csv("Data_cleaned.csv")

data_project3_1[['MachineIdentifier','HasDetections','DANGEROUS_RAM_INSTALLED','DANGEROUS_PROCESSOR_INSTALLED','DANGEROUS_PROCESSOR_TYPE_INSTALLED','DANGEROUS_DISPLAY_SIZE','HIGH_END_COMPUTER', 'HAS_ONLY_ONE_AV',
       'HAS_AN_OS_WSUPPORT','Wdft_IsGamer','Census_GenuineStateName','Census_HasOpticalDiskDrive','Census_PrimaryDiskTypeName','Firewall','HasTpm','Platform','OsVer','Census_MDC2FormFactor','AVProductsEnabled','Census_TotalPhysicalRAM','filled_ProcessorCoreCount','Census_PrimaryDiskTotalCapacity','Census_InternalBatteryNumberOfCharges','Census_SystemVolumeTotalCapacity','Census_InternalPrimaryDiagonalDisplaySizeInInches','Census_InternalPrimaryDisplayResolutionHorizontal','Census_InternalPrimaryDisplayResolutionVertical']].to_csv("Data_to_model.csv")


data_project3_1.columns
titanic_1_5=data_project3_1

titanic_1_5[['AVProductsEnabled','Census_TotalPhysicalRAM','filled_ProcessorCoreCount','Census_PrimaryDiskTotalCapacity','Census_InternalBatteryNumberOfCharges','Census_SystemVolumeTotalCapacity','Census_InternalPrimaryDiagonalDisplaySizeInInches','Census_InternalPrimaryDisplayResolutionHorizontal','Census_InternalPrimaryDisplayResolutionVertical']].info()
titanic_1_6=titanic_1_5[['AVProductsEnabled','Census_TotalPhysicalRAM','filled_ProcessorCoreCount','Census_PrimaryDiskTotalCapacity','Census_InternalBatteryNumberOfCharges','Census_SystemVolumeTotalCapacity','Census_InternalPrimaryDiagonalDisplaySizeInInches','Census_InternalPrimaryDisplayResolutionHorizontal','Census_InternalPrimaryDisplayResolutionVertical']].quantile(q=np.arange(0,1.1,0.1),axis=0)

def intervals_creator(titanic_1_6,titanic_1_5):
   result=[]
   for i in titanic_1_6.columns: 
      # print(i)
      for j in np.arange(0,(titanic_1_6[i].count()-1),1):
         if  (titanic_1_6[i].iloc[j]!=titanic_1_6[i].iloc[j+1]) :  
            # print(titanic_1_6[i].iloc[j])
            # print(titanic_1_6[i].iloc[j+1])   
            if (j!=titanic_1_6[i].count()-1):
               titanic_1_5_mod=titanic_1_5[['MachineIdentifier','HasDetections',i]][(titanic_1_5[i]<titanic_1_6[i].iloc[j+1]) & (titanic_1_5[i]>=titanic_1_6[i].iloc[j]) ].groupby(['HasDetections']).count()
            else :
               titanic_1_5_mod=titanic_1_5[['MachineIdentifier','HasDetections',i]][(titanic_1_5[i]<=titanic_1_6[i].iloc[j+1]) & (titanic_1_5[i]>=titanic_1_6[i].iloc[j]) ].groupby(['HasDetections']).count()
            # titanic_1_5_mod['MachineIdentifier'].loc[1]/titanic_1_5[['HasDetections']].sum()
            # titanic_1_5_mod['MachineIdentifier'].loc[0]/(titanic_1_5[['MachineIdentifier','HasDetections',i]].shape[0]-titanic_1_5[['HasDetections']].sum())

            result.append([i,"".join(["[",(titanic_1_6[i].iloc[j]).astype(str),"-",(titanic_1_6[i].iloc[j+1]).astype(str),")"]),(titanic_1_5_mod['MachineIdentifier'].loc[1]),(titanic_1_5_mod['MachineIdentifier'].loc[0]),(titanic_1_5_mod['MachineIdentifier'].loc[1]/titanic_1_5[['HasDetections']].sum()).astype(float)[0],(titanic_1_5_mod['MachineIdentifier'].loc[0]/(titanic_1_5[['MachineIdentifier','HasDetections',i]].shape[0]-titanic_1_5[['HasDetections']].sum())).astype(float)[0]])
   
   result1=pd.DataFrame(result)
   result1.columns=['feature','interval','count_1','count_0','Percent_1','Percent_0']
   return(result1)   


gini_table=intervals_creator(titanic_1_6,titanic_1_5)

gini_table.to_excel("Intervals_for_Numericals2.xlsx")


gini_calculated=[]
#i='SibSp'
for i in set(gini_table['feature']):
   print(gini_table[gini_table['feature']==i])
   gini_calculated1=[]
   gini_table_1=(gini_table[gini_table['feature']==i]['Percent_1']).agg({'Percent_1':np.cumsum})
   gini_table_0=(gini_table[gini_table['feature']==i]['Percent_0']).agg({'Percent_0':np.cumsum})
   for j in np.arange(0,(gini_table[gini_table['feature']==i].shape[0]),1):
      if (j==0):
         gini_calculated1.append((gini_table_1.iloc[j])*(gini_table_0.iloc[j]))
      else: 
         gini_calculated1.append(((gini_table_0.iloc[j]-(gini_table_0.iloc[j-1]))*((gini_table_1.iloc[j]+gini_table_1.iloc[j-1]))))
      
   gini_calculated.append([i,np.sum(gini_calculated1),abs(1-np.sum(gini_calculated1))])

gini_for_numerics=pd.DataFrame(gini_calculated)
gini_for_numerics.columns=['feature','calculation_gini','gini']

gini_for_numerics.to_excel("Gini_table_for_numericals2.xlsx")


Correlation_analysis=titanic_1_5[['AVProductsEnabled','Census_TotalPhysicalRAM','filled_ProcessorCoreCount','Census_PrimaryDiskTotalCapacity','Census_InternalBatteryNumberOfCharges','Census_SystemVolumeTotalCapacity','Census_InternalPrimaryDiagonalDisplaySizeInInches','Census_InternalPrimaryDisplayResolutionHorizontal','Census_InternalPrimaryDisplayResolutionVertical']].corr()
Correlation_analysis.to_excel("Correlation_matrix2.xlsx")

#Just 6 numeric variables survived if we use the threshold 5% of gini


gini_table['feature']
numeric_variable_selector=['Census_TotalPhysicalRAM',
'filled_ProcessorCoreCount',
'Census_PrimaryDiskTotalCapacity',
'Census_InternalBatteryNumberOfCharges',
'Census_SystemVolumeTotalCapacity',
'Census_InternalPrimaryDiagonalDisplaySizeInInches',
'Census_InternalPrimaryDisplayResolutionHorizontal']


gini_table['LN']=np.log10(gini_table['count_1']/gini_table['count_0'])
gini_table['count_1_cumsum']=gini_table[['feature','count_1']].groupby('feature').cumsum()
gini_table['count_0_cumsum']=gini_table[['feature','count_0']].groupby('feature').cumsum()
(gini_table[['feature','count_0_cumsum']].groupby('feature').max()).columns
intermediate=gini_table.groupby(['feature']).agg({'count_0':np.sum})
intermediate2=pd.merge(gini_table,intermediate,on='feature',how='left')
intermediate2_1=gini_table.groupby(['feature']).agg({'count_1':np.sum})
intermediate3=pd.merge(intermediate2,intermediate2_1,on='feature',how='left')

gini_table2=intermediate3
gini_table2.columns
gini_table2['WOE']=np.log10((gini_table2['count_0_cumsum']/gini_table2['count_0_y'])/(gini_table2['count_1_cumsum']/gini_table2['count_1_y']))
gini_table2['IV']=((gini_table2['count_0_cumsum']/gini_table2['count_0_y'])-(gini_table2['count_1_cumsum']/gini_table2['count_1_y']))*gini_table2['LN']
gini_table2.groupby(['feature']).agg({'IV':np.sum})
#the IV as the gini criteria should be modified in order to select some variablees (Select a lower threshold)

data_project_for_modelling=data_project3_1['MachineIdentifier']




gini_table[['feature','count_1_cumsum']].groupby('feature').max()
gini_table.columns

gini_table['count_1_TOTAL']

print(gini_table)
data_project3_1.columns


wdf[(wdf["HasDetections"] == 1)]["HasDetections"].count().sum()
#data_project3_1["HasDetections"][data_project3_1["HasDetections"] == 1].sum()

wdf = data_project3_1[['HasDetections','DANGEROUS_RAM_INSTALLED','DANGEROUS_PROCESSOR_INSTALLED','DANGEROUS_PROCESSOR_TYPE_INSTALLED','DANGEROUS_DISPLAY_SIZE','HIGH_END_COMPUTER', 'HAS_ONLY_ONE_AV',
       'HAS_AN_OS_WSUPPORT','Wdft_IsGamer','Census_GenuineStateName','Census_HasOpticalDiskDrive','Census_PrimaryDiskTypeName','Firewall','HasTpm','Platform','OsVer','Census_MDC2FormFactor']]

# WOE = LN(relative_frequency_of_goods/relative_frequency_of_bads) (goods or events are the objetive of the analysis - has a malware)
# IV = SUM(NonEventsRate - EventsRate) * WOE  
# This website explains the formulas in more detail: http://rstudio-pubs-static.s3.amazonaws.com/216381_5d8fab3c7de64cb99c6f33503d6c7195.html
# Generally, the lower the IV, the less useful it is for predictive power 

# Setting up a new dataframe 
univariate = pd.DataFrame(columns =["Variable", "Category", "Count", "Event", "NonEvent", "EventRate", "NonEventRate", "logit_transformation" ,"WOE", "IV"])
A, B, C, D, E = [], [], [], [], []
ind = 0

# Applying the WOE and IV calculations 
for i in wdf.columns:
    IV = 0 
    list_variables = wdf[i].unique()
    cleanedvariableList = [x for x in list_variables if str(x) != 'nan']
    TOTALEVENT = wdf.shape[0]#wdf[(wdf["HasDetections"] == 1)]["HasDetections"].count().sum()
    for j in cleanedvariableList: 
        COUNT = wdf[(wdf[i] == j)]["HasDetections"].count().sum()
        EVENT = wdf[(wdf["HasDetections"] == 1) & (wdf[i] == j)]["HasDetections"].count().sum()
        NONEVENT = COUNT - EVENT 
        percent_EVENT = EVENT/TOTALEVENT
        percent_EVENT_relative=EVENT/(wdf[(wdf["HasDetections"] == 1)]["HasDetections"].count().sum())
        percent_NONEVENT_relative= NONEVENT / (TOTALEVENT-wdf[(wdf["HasDetections"] == 1)]["HasDetections"].count().sum())
        percent_NONEVENT = NONEVENT/TOTALEVENT
        logit_transformation = np.log10(percent_EVENT/percent_NONEVENT)
        WOE=np.log10(percent_NONEVENT_relative/percent_EVENT_relative)
        IV = (percent_NONEVENT_relative - percent_EVENT_relative)*WOE
        ind +=1
        univariate.loc[ind] = [i, j, COUNT, EVENT, NONEVENT, percent_EVENT, percent_NONEVENT,logit_transformation, WOE, IV]
    for l in cleanedvariableList: 
        univariate.set_value(univariate[(univariate["Category"] == l)].index[0], "IV", IV)
    
    # setting up bins to pick good variables
    if str(IV)  == 'inf': # for now I left out any IV that equalled to infinity (this just means that event = 0 leading to WOE = x/0 = inf)  
        continue 
    if IV < 0.02:
        A.append(i)
    elif IV <0.1: 
        B.append(i)
    elif IV <0.3: 
        C.append(i)
    elif IV <0.5:
        D.append(i)
    else: 
        E.append(i)

print("Very weak prediction: \t", A, "\nWeak predictor: \t", B, "\nMedium predictor: \t", C,"\nStrong predictor: \t", D, "\nVery Strong predictor: \t", E)

univariate

#Just pass 'HAS_ONLY_ONE_AV' using IV 

univariate.to_excel("Revision_of_IVs.xlsx")
###########################################################################################################################################################



